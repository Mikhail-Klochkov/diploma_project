{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 19:28:37.237143 139699871221568 backprop.py:995] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0411 19:28:37.238929 139699871221568 backprop.py:995] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0411 19:28:37.243899 139699871221568 backprop.py:995] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0411 19:28:37.245912 139699871221568 backprop.py:995] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0411 19:28:37.246978 139699871221568 backprop.py:995] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " __init__:  <tf.Variable 'Variable:0' shape=(2,) dtype=float64, numpy=array([1.3, 0.7])> <function rosen_tf at 0x7f0d07ee80d0> (2,) \n",
      " end __init__\n",
      "tensor:  <tf.Variable 'Variable:0' shape=(2,) dtype=float64, numpy=array([2., 2.])>\n",
      "current value of x:  [1.3 0.7]\n",
      "current value of x:  [0.4218575  0.04762861]\n",
      "current value of x:  [0.42750931 0.14498137]\n",
      "current value of x:  [0.41673545 0.16652911]\n",
      "current value of x:  [0.41507145 0.1698571 ]\n",
      "current value of x:  [0.41496936 0.17006128]\n",
      "current value of x:  [0.41494931 0.17010138]\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 14, function evaluations: 10, CG iterations: 9, optimality: 2.99e-09, constraint violation: 0.00e+00, execution time: 0.23 s.\n",
      "[0.41494531 0.17010937]\n",
      " __init__:  <tf.Variable 'Variable:0' shape=(2,) dtype=float64, numpy=array([-1., -1.])> <function _funct_ at 0x7f0d07bf7950> (2,) \n",
      " end __init__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=1.0>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "@tf.function\n",
    "def input_tens(tens):\n",
    "    tf.print(tens)\n",
    "\n",
    "def rosen(x): # numpy example of function\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return np.sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0, axis=0)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def input_(x_tens):\n",
    "    tf.print(x_tens)\n",
    "\n",
    "def rosen_tf(x_tens):\n",
    "    return tf.reduce_sum(100.0 * (x_tens[1:] - x_tens[:-1] ** 2) ** 2 + (1 - x_tens[:-1]) ** 2)\n",
    "\n",
    "\n",
    "## second_variant ##\n",
    "\n",
    "def _funct_(x_tens): # takes a tensor type object\n",
    "    return 0.01 * x_tens[0] ** 2 + x_tens[1] ** 2 - 100.\n",
    "\n",
    "## define a nonlinear condition functions where is we translate a tensortype\n",
    "\n",
    "def cons_1(x):\n",
    "    return x[0] ** 2 + x[1]\n",
    "\n",
    "def cons_2(x):\n",
    "    return x[0] ** 2 - x[1]\n",
    "\n",
    "list_of_nonlinear_functions = [cons_1, cons_2] #\n",
    "\n",
    "## ________end_________\n",
    "\n",
    "def _funct_2(x):\n",
    "    assert(x.shape[0] == 4 and x.shape.__len__() == 1), ('Shape is Wrong!')\n",
    "    return x[0] ** 2 + x[1] ** 2 + 2 * x[2] ** 2 + x[3] ** 2 -  5 * x[0] - 5 * x[1] - 21 * x[2] + 7 * x[3]\n",
    "\n",
    "def non_1(x):\n",
    "    assert(x.shape[0] == 4 and x.shape.__len__() == 1), ('Shape is Wrong!')\n",
    "    return - x[0] ** 2 - x[1] ** 2 - x[2] ** 2 - x[3] ** 2 - x[0] + x[1] - x[2] + x[3] + 8\n",
    "\n",
    "def non_2(x):\n",
    "    assert(x.shape[0] == 4 and x.shape.__len__() == 1) ('Shape is Wrong!')\n",
    "    return - x[0] ** 2 - 2 * x[1] ** 2 - x[2] ** 2 - 2 * x[3] ** 2 + x[0] + x[3] + 10\n",
    "\n",
    "def non_3(x):\n",
    "    assert(x.shape[0] == 4 and x.shape.__len__() == 1) ('Shape is Wrong!')\n",
    "    return - 2 * x[0] ** 2 - x[1] ** 2 - x[2] ** 2 - 2 * x[0] + x[1] + x[3] +  5\n",
    "\n",
    "##\n",
    "#list_of_nonlinear_functions = [non_1, non_2, non_3]\n",
    "\n",
    "class Nonlinear_constraints(object):\n",
    "\n",
    "    def __init__(self, x_tensor): # we transfer a x_tensor\n",
    "        global list_of_nonlinear_functions\n",
    "        self._stack = []\n",
    "        ## Here we should write a all nonlinear functions:\n",
    "        assert (x_tensor.shape.__len__() == 1) , ('shape is not signature: (n, )!')\n",
    "\n",
    "\n",
    "        self._tensor = x_tensor\n",
    "        print(\"tensor: \", x_tensor.__repr__())\n",
    "        # we work with 2 dims\n",
    "        for iter_ in range(len(list_of_nonlinear_functions)):\n",
    "            self._stack.append(list_of_nonlinear_functions[iter_])\n",
    "\n",
    "        self._s = len(self._stack)\n",
    "\n",
    "    def wrapper_conditions_(self):\n",
    "        def _inner_(vars_x) -> float:\n",
    "            ## Вообще нет смысла обновлять значения\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0] or vars_x.shape.__len__() != 1):\n",
    "                assert (False) ('shape is wrong!')\n",
    "\n",
    "            vars_tf = tf.convert_to_tensor(vars_x)\n",
    "            stack_return = []\n",
    "            for iter_ in range(self._s):\n",
    "                stack_return.append(self._stack[iter_](vars_tf))\n",
    "            return stack_return # Так или иначе, это стэк list из объектов типа тензоров\n",
    "        return _inner_\n",
    "\n",
    "\n",
    "    def jacobian_of_constraints(self):\n",
    "        global list_of_nonlinear_functions # Тут у нас лежат наши функции нелинейные\n",
    "        stack_ = []\n",
    "        with tf.GradientTape(persistent = True) as g:\n",
    "                g.watch((self._tensor))\n",
    "                for iter_ in range(self._s):\n",
    "                    fn_ = list_of_nonlinear_functions[iter_](self._tensor)\n",
    "                    stack_.append(g.gradient(fn_ , self._tensor))\n",
    "\n",
    "        return stack_\n",
    "\n",
    "    def wrapper_jacobian(self):\n",
    "        def _inner_(vars_x) -> float: # vars_s should be a np.array object\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0] or vars_x.shape.__len__() != 1):\n",
    "                assert (False) ('shape is wrong!')\n",
    "            vars_tf = tf.convert_to_tensor(vars_x)\n",
    "            self._tensor = vars_tf # we should to change, because  we call jacobian method with automaticly computetion jacobian with self._tensor\n",
    "\n",
    "            return self.jacobian_of_constraints()\n",
    "\n",
    "        return _inner_\n",
    "\n",
    "    def hessian_of_constraints(self):\n",
    "        global list_of_nonlinear_functions\n",
    "        matrix_total = []\n",
    "        for iter_ in range(self._s): # по всем таким функциям делаем проход для рассчёта Hessiana\n",
    "            matrix_current = []\n",
    "            with tf.GradientTape(persistent = True) as g: # В рамках одной такой\n",
    "                g.watch(self._tensor)\n",
    "                fn_current = list_of_nonlinear_functions[iter_](self._tensor)\n",
    "                grad_ = g.gradient(fn_current, self._tensor)\n",
    "                list_current = []\n",
    "                for jter_ in range(self._tensor.shape[0]):\n",
    "                    grad_grad_ = g.gradient(grad_[jter_], self._tensor)\n",
    "                    list_current.append(grad_grad_)\n",
    "\n",
    "                matrix_current.append(list_current) # На выходе должна быть матрица n*n от размерности\n",
    "\n",
    "            matrix_total.append(matrix_current)\n",
    "            matrix_total_tf = tf.convert_to_tensor(matrix_total)\n",
    "\n",
    "        new_matrix_tf = [] # Чтобы убрать одну размерность!\n",
    "        for iter_ in range(self._s):\n",
    "            new_matrix_tf.append(matrix_total_tf[iter_][0])\n",
    "\n",
    "        return new_matrix_tf # Он возвращает лист с перечисленными тензорами, которые есть Гессианы для каждой нелинейной функции ограничения\n",
    "\n",
    "    def wrapper_hessians_matrix(self):\n",
    "        def _inner_(vars_x) -> float:\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0] or vars_x.shape.__len__() != 1):\n",
    "                assert (False) ('shape is wrong!')\n",
    "            vars_tf = tf.convert_to_tensor(vars_x)\n",
    "            self._tensor = vars_tf\n",
    "\n",
    "            return self.hessian_of_constraints() # list of tensors hessians\n",
    "        return _inner_\n",
    "\n",
    "\n",
    "class function(tf.Module):\n",
    "\n",
    "    # аттрибут для вывода значений вектора x - как он у нас меняется\n",
    "    # в процессе иттераций алгоритма scipy\n",
    "\n",
    "    def compute_gradient_optional(self, functional_obj):\n",
    "            with tf.GradientTape() as g:\n",
    "                g.watch(self._tensor)\n",
    "                res = functional_obj(self._tensor)\n",
    "                grad_ = g.gradient(res, self._tensor)\n",
    "\n",
    "            return grad_\n",
    "\n",
    "    def __init__(self, x_numpy, functional_obj):\n",
    "        if(x_numpy.shape.__len__() != 1):\n",
    "            assert (1 != 1) ('shape is not signature : (n, )!')\n",
    "        else:\n",
    "            print(\" __init__: \", x_numpy, functional_obj, x_numpy.shape, '\\n end __init__')\n",
    "            self._tensor = tf.Variable(x_numpy)\n",
    "            self._times_call_change = 0\n",
    "            self._functional_obj = functional_obj # тензорная функция\n",
    "\n",
    "    def __call__(self, some_parameter = 0): # __call__ function and convert in tensor\n",
    "        return self._functional_obj(self._tensor) # we call a function object\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (self._tensor.__repr__(), self._functional_obj.__repr__())\n",
    "\n",
    "    def _change(self, new_vector):\n",
    "        self._times_call_change += 1\n",
    "        @tf.function\n",
    "        def assign_(x_new):\n",
    "            x_new_tensor = tf.convert_to_tensor(x_new)\n",
    "            if(x_new_tensor.shape != self._tensor.shape):\n",
    "                assert (1 != 1) (\"shape of tensor is not a equal!\")\n",
    "            else:\n",
    "                self._tensor.assign(x_new_tensor)\n",
    "        # для вывода текущих значений\n",
    "        if(self._times_call_change % 4 == 0):\n",
    "            print('current value of x: ', self._tensor.numpy())\n",
    "\n",
    "        assign_(new_vector)\n",
    "\n",
    "    def _wrapper(self): # Обёртка вокруг нашей целевой функции\n",
    "        def _inner(vars_x) -> np.float32:\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0]):\n",
    "                assert (1 != 1) ('Error with different size!: ')\n",
    "                return None\n",
    "            else:\n",
    "                self._change(vars_x) # we change aor vector of x\n",
    "                return (self.__call__(some_parameter = 0)).numpy()\n",
    "\n",
    "        return _inner\n",
    "\n",
    "    def compute_gradient(self):\n",
    "                with tf.GradientTape() as g:\n",
    "                    g.watch((self._tensor))\n",
    "                    f_a = self._functional_obj(self._tensor)\n",
    "                    grad_ = g.gradient(f_a, self._tensor)\n",
    "\n",
    "                return grad_\n",
    "\n",
    "    def wrapper_grad(self):\n",
    "        def inner_grad(vars_x, *args) -> float: # should be a vector of shape np.float\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0]):\n",
    "                assert (1 != 1) ('Error with different size!: ')\n",
    "                return None\n",
    "            else:\n",
    "                self._change(vars_x)\n",
    "                return self.compute_gradient().numpy()\n",
    "\n",
    "        return inner_grad\n",
    "\n",
    "    def compute_hessian(self):\n",
    "        with tf.GradientTape(persistent = True) as g:\n",
    "            g.watch(self._tensor)\n",
    "            f_a = self._functional_obj(self._tensor)\n",
    "            #f_a = tf.reduce_sum(100.0 * (self._tensor[1:] - self._tensor[:-1] ** 2) ** 2 + (1 - self._tensor[:-1]) ** 2)\n",
    "            grad_ = g.gradient(f_a, self._tensor)\n",
    "            list_ = []\n",
    "            for iter_ in range(self._tensor.shape[0]):\n",
    "                grad_grad = g.gradient(grad_[iter_], self._tensor)\n",
    "                list_.append(grad_grad)\n",
    "\n",
    "            matrix = tf.convert_to_tensor(list_)\n",
    "        return matrix\n",
    "\n",
    "\n",
    "    def wrapper_hessian(self):\n",
    "        def inner_hessian(vars_x, *args) -> float:\n",
    "            if(vars_x.shape[0] != self._tensor.shape[0]):\n",
    "                assert (1 != 1) ('Error with different size!: ')\n",
    "                return None\n",
    "            else:\n",
    "                self._change(vars_x)\n",
    "                return self.compute_hessian().numpy()\n",
    "\n",
    "        return inner_hessian\n",
    "\n",
    "\n",
    "x_tens_ = tf.Variable(np.array([1.3, 0.7]))\n",
    "foo_ = function(x_tens_, rosen_tf)\n",
    "function_ = foo_._wrapper()\n",
    "funct_grad = foo_.wrapper_grad()\n",
    "funct_hessian = foo_.wrapper_hessian()\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "x_0 = np.array([1.3 , 0.7])\n",
    "# Пример поиска минимума тремя разными методам без ограничений\n",
    "#res_ = minimize(function_, x_0, method = 'nelder-mead', options = {'xtol': 1e-8, 'disp': True})\n",
    "#print(res_.x)\n",
    "#res_an_ = minimize(function_, x_0, method = 'BFGS', jac = funct_grad, options = {'disp': False})\n",
    "#print(res_an_)\n",
    "#res_another = minimize(function_, x_0, method = 'Newton-CG', jac = funct_grad,hess = funct_hessian,options = {'xtol' : 1e-8, 'disp': True})\n",
    "#print(res_another)\n",
    "# Пример задачи минимизации уже с ограничениями как в статье на scipy.optimize\n",
    "\n",
    "\n",
    "x_ = tf.Variable(np.full((2, ), 2.), dtype = tf.float64)\n",
    "nonlinear_constraints = Nonlinear_constraints(x_) # create a object Nonlinear\n",
    "\n",
    "def wr_cons_f(nonlinear_constraints):\n",
    "    def cons_f(x):\n",
    "        wrapper_obj = nonlinear_constraints.wrapper_conditions_()\n",
    "        wrapper_obj = list(map(lambda x: x.numpy(), wrapper_obj(x)))\n",
    "        return wrapper_obj\n",
    "    return cons_f\n",
    "\n",
    "def wr_cons_J(nonlinear_constraints):\n",
    "    def cons_J(x):\n",
    "        funct_obj = nonlinear_constraints.wrapper_jacobian()\n",
    "        funct_obj = list(map(lambda x: list(x.numpy()), funct_obj(x)))\n",
    "        return funct_obj\n",
    "    return cons_J\n",
    "\n",
    "def wr_cons_H(nonlinear_constraints):\n",
    "    def cons_H(x, v): # x - np.array() v - np.array() also\n",
    "        if(nonlinear_constraints._s != v.shape[0] or v.shape.__len__() != 1):\n",
    "            assert(False) ('shape of v vector is wrong!')\n",
    "\n",
    "        wrapper_obj = nonlinear_constraints.wrapper_hessians_matrix()\n",
    "        wrapper_obj = wrapper_obj(x) # _inner_(vars_x) -> float: по факту тут вычисленный list с tensors hessians\n",
    "        total_ = 0 # H(x, v) как в статье\n",
    "        for iter_ in range(nonlinear_constraints._s):\n",
    "            total_ += v[iter_] * wrapper_obj[iter_].numpy()\n",
    "\n",
    "        return total_\n",
    "    return cons_H\n",
    "\n",
    "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
    "linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1],\n",
    "                                     [1,1])\n",
    "nonlinear_constraints_ = NonlinearConstraint(wr_cons_f(nonlinear_constraints), -np.inf, 1,\n",
    "                                             jac = wr_cons_J(nonlinear_constraints), hess = wr_cons_H(nonlinear_constraints))\n",
    "\n",
    "res_constraints_ = minimize(function_, x_0, method = 'trust-constr', jac = funct_grad,\n",
    "               hess = funct_hessian,\n",
    "               constraints = [linear_constraint, nonlinear_constraints_],\n",
    "               options = {'verbose': 1}, bounds = bounds)\n",
    "print(res_constraints_.x)\n",
    "\n",
    "bounds_2 = Bounds([2., 50.], [-50., 50.])\n",
    "linear_constraint_2 = LinearConstraint([10, -1], 10, +np.inf)\n",
    "\n",
    "x_tens_2  = tf.Variable(np.array([-1, -1], dtype = np.float64))\n",
    "foo_2 = function(x_tens_2, _funct_)\n",
    "function_2 = foo_2._wrapper()\n",
    "funct_grad_2 = foo_2.wrapper_grad()\n",
    "funct_hessian_2 = foo_2.wrapper_hessian()\n",
    "\n",
    "x_0_2 = x_tens_2.numpy()\n",
    "\n",
    "#res_constraints_2 = minimize(function_2, x_0_2, method = 'trust-constr',\n",
    "#                            jac = funct_grad_2, hess = funct_hessian_2,\n",
    "#                            constraints = [linear_constraint_2],\n",
    "#                            options = {'verbose': 1}, bounds = bounds_2)\n",
    "\n",
    "#print(res_constraints_2.x)\n",
    "#foo_2._functional_obj(tf.constant(np.array([2, 0], dtype = np.float64)))\n",
    "\n",
    "\n",
    "rosen_tf(tf.constant([2.,4.], dtype = tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\min\\limits_{ w } w^{T}\\Sigma w \\\\\n",
    "\\sum_{i = 1}^{n} r_{i}w_{i}  \\geq q  \\\\\n",
    "\\sum_{i = 1}^{n} w_{i} = 1 \n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "Можно, также, помимо линейных ограничений ввести ещё и bounds conditions. Разрешив нашим весовым параметрам быть только положительными! \n",
    "\\begin{equation}\n",
    "w_{i} \\geq 0 , \\;\\;\\;\\; i = 1..n\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "#############\n",
    "key_const = 'f8nzs159xkB-_NJg4LCB'\n",
    "dict_of_securities = ['CNP', 'F', 'WMT', 'GE', 'TSLA'] #by default\n",
    "quandl.ApiConfig.api_key = key_const\n",
    "trading_days = 252\n",
    "#############\n",
    "\n",
    "\n",
    "## Надо его адаптировать под класс функций, чтобы формат записи был один и тот же ## \n",
    "## У нас для создания объекта класса function необходим массив строчный типа tf.Variable\n",
    "## Ф также функция с сигнатурой foo(tensor_type_obj) : -> tensor_type_obj для нашего функционала\n",
    "## Пока без ограничений при этом мы передадим в наш класс только initial_value для дальнейшего изменения\n",
    "\n",
    "class Markovitz_theory(object):\n",
    "    global key_const # my key for access to data\n",
    "    global dict_of_securities\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.flag = False # По сути детектирует то, что пока мы не вызвали метод _initial_model_param_\n",
    "        \n",
    "        self.weights = None\n",
    "        self.cov_daily_tf = None\n",
    "        self.returns_annual_tf = None\n",
    "        \n",
    "        self.data = quandl.get_table('WIKI/PRICES', ticker = dict_of_securities,\n",
    "                        qopts = { 'columns': ['date', 'ticker', 'adj_close'] },\n",
    "                        date = { 'gte': '2014-1-1', 'lte': '2016-12-31' }, paginate = True)\n",
    "        self.data = self.data.set_index('date')\n",
    "        self.data = self.data.pivot(columns = 'ticker')\n",
    "    \n",
    "    def _initial_model_param_(self):\n",
    "        if self.flag == False:\n",
    "            \n",
    "            num_of_securities = dict_of_securities.__len__()\n",
    "            daily_returns_data = self.data.pct_change()\n",
    "            \n",
    "            self.returns_annual_tf = tf.constant((daily_returns_data.mean() * trading_days)[:, np.newaxis], dtype = tf.float64) # maybe should be a dtype = tf.float32/tf.float64\n",
    "            self.cov_daily_tf = tf.constant((daily_returns_data.cov() * trading_days).values, dtype = tf.float64) # matrix of covariation between different columns\n",
    "            \n",
    "            #cov_annual = cov_daily.mean() * trading_days # Это пока не используется\n",
    "            ## initial weigthts ## По простому это наши неизвестные, по которым у нас распределены доходы\n",
    "            ##  он даёт только положительные веса, поэтму  в принципе стоит добавить ограничения\n",
    "            weights = np.random.random(num_of_securities) # Мы тут выбираем равномерное распределение [0,1]\n",
    "            \n",
    "            self.weights = tf.Variable((weights / np.sum(weights))[:, np.newaxis], dtype = tf.float64, name = 'weights')      \n",
    "            self.flag = True\n",
    "            print(\"initial_weights w: {!s}\".format(self.weights.numpy())) # call only one time\n",
    "    \n",
    "    def _Markovitz_functionals_(self): # weights should be a tensor для реализации functionS \n",
    "        ### Здесь мы проинтициализируем параметры модели Марковица для определения функционала(!уровня риска!) - мы его минимизируем\n",
    "        if(self.flag == False): # Я не задад параметры и начальные веса модели, то мы их инициализируем \n",
    "            self._initial_model_param_()\n",
    "        ### тут мы обменяем два тензорных объекта self.weights <-> weights_tf ###\n",
    "        ### weights_tf - should be a shape (num, ) -> ###\n",
    "        return tf.sqrt(tf.matmul(tf.transpose(self.weights), tf.matmul(self.cov_daily_tf, self.weights)))[0][0]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"r: \\n {!s} \\n W:\\n {!s} \\n COV(r_i, r_j):\\n {!s} \\n\".format(self.returns_annual_tf.numpy(), self.weights.numpy(),\n",
    "                                                                 self.cov_daily_tf.numpy())\n",
    "    # наша модель включает ограничения только типа равенств и неравенств линейного типа. Данная функция будет принимать нижнею\n",
    "    # Границу наших рисков и минимизировать deviation\n",
    "    def _linear_constraints_(self, Q):\n",
    "        linear_constraints = LinearConstraint([list(self.returns_annual_tf.numpy()[:, 0]), # вроде в виде list мы должны засунуть\n",
    "                                               list(np.ones(self.returns_annual_tf.shape[0]))],\n",
    "                                               [Q, 1.], [np.inf, 1.])\n",
    "        return linear_constraints\n",
    "    \n",
    "    def _bounds_(self):\n",
    "        bounds = Bounds([0.0] * self.weights.shape[0], [np.inf] * self.weights.shape[0])\n",
    "        return bounds\n",
    "    \n",
    "    def __return_total__(self):\n",
    "        return tf.matmul(tf.transpose(self.weights), self.returns_annual_tf)[0][0]\n",
    "    \n",
    "    def _volatility_(self):\n",
    "        return tf.sqrt(tf.matmul(tf.transpose(self.weights), tf.matmul(self.cov_daily_tf, self.weights)))[0][0]\n",
    "    \n",
    "def _wrapper_marcovitz(model_mark):\n",
    "    cov_matrix = model_mark.cov_daily_tf\n",
    "    def _Marcovitz_risks(weights_tf):\n",
    "        if(weights_tf.shape.__len__() != 2):\n",
    "            weights_tf = weights_tf[:, np.newaxis]\n",
    "\n",
    "        return tf.matmul(tf.transpose(weights_tf), tf.matmul(cov_matrix, weights_tf))[0][0]\n",
    "    return _Marcovitz_risks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_weights w: [[0.04604615]\n",
      " [0.25531798]\n",
      " [0.19404603]\n",
      " [0.22431059]\n",
      " [0.28027926]]\n"
     ]
    }
   ],
   "source": [
    "### Создание объекта модели и расчет начальных весов и параметров портфеля Марковица на основе данных, полученных из Quandl ###\n",
    "model_ = Markovitz_theory() # создаём объект, который содержит наш функционал и параметры модели\n",
    "model_._initial_model_param_() # Инициализация начальных параметров, делается только один раз  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.029528066306234134>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functionals_Markovitz = _wrapper_marcovitz(model_)\n",
    "functionals_Markovitz(model_.weights[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_mark = model_._Markovitz_functionals_\n",
    "def __wrapper__(param):\n",
    "    global foo_\n",
    "\n",
    "__wrapper__(model_._Markovitz_functionals_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " __init__:  [3.04846213e-01 9.91402087e-05 4.23385093e-01 2.71508071e-01\n",
      " 1.61483224e-04] <function _wrapper_marcovitz.<locals>._Marcovitz_risks at 0x7f0d07cd0378> (5,) \n",
      " end __init__\n",
      "current value of x:  [3.04846213e-01 9.91402087e-05 4.23385093e-01 2.71508071e-01\n",
      " 1.61483224e-04]\n",
      "current value of x:  [0.34746121 0.15650566 0.38247055 0.22759996 0.21845815]\n",
      "current value of x:  [0.25838197 0.13964252 0.27190574 0.25487723 0.16168364]\n",
      "current value of x:  [0.25097444 0.13844245 0.25837763 0.2924735  0.15750727]\n",
      "current value of x:  [0.25178902 0.07298688 0.25893936 0.35373842 0.09133163]\n",
      "current value of x:  [0.17570559 0.03118459 0.18769369 0.57734686 0.03176696]\n",
      "current value of x:  [0.18406169 0.0278732  0.19723982 0.55967282 0.03115248]\n",
      "current value of x:  [0.18953933 0.00276335 0.3050667  0.499739   0.00289161]\n",
      "current value of x:  [0.20930014 0.00096261 0.29044618 0.49768779 0.00160328]\n",
      "current value of x:  [2.09912077e-01 3.03492679e-04 2.94215437e-01 4.95430261e-01\n",
      " 1.38731332e-04]\n",
      "current value of x:  [2.09713769e-01 1.40086353e-05 2.95569338e-01 4.94696653e-01\n",
      " 6.23141987e-06]\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 22, function evaluations: 15, CG iterations: 19, optimality: 7.68e-10, constraint violation: 1.11e-16, execution time: 0.23 s.\n",
      "_output_:   barrier_parameter: 1.2800000000000007e-06\n",
      " barrier_tolerance: 1.2800000000000007e-06\n",
      "          cg_niter: 19\n",
      "      cg_stop_cond: 4\n",
      "            constr: [array([0.1500015, 1.       ]), array([2.09713769e-01, 1.40086353e-05, 2.95569338e-01, 4.94696653e-01,\n",
      "       6.23141987e-06])]\n",
      "       constr_nfev: [0, 0]\n",
      "       constr_nhev: [0, 0]\n",
      "       constr_njev: [0, 0]\n",
      "    constr_penalty: 1.0\n",
      "  constr_violation: 1.1102230246251565e-16\n",
      "    execution_time: 0.23275279998779297\n",
      "               fun: 0.06048757178392171\n",
      "              grad: array([0.0437962 , 0.04788958, 0.04859143, 0.19694397, 0.02345205])\n",
      "               jac: [array([[ 9.25938137e-02, -7.41686372e-03,  9.61619852e-02,\n",
      "         2.06512159e-01, -9.50830068e-04],\n",
      "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
      "         1.00000000e+00,  1.00000000e+00]]), array([[1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.]])]\n",
      "   lagrangian_grad: array([ 7.23601624e-10,  9.25887145e-12, -7.67561402e-10,  3.85833623e-11,\n",
      "       -3.88253318e-12])\n",
      "           message: '`gtol` termination condition is satisfied.'\n",
      "            method: 'tr_interior_point'\n",
      "              nfev: 15\n",
      "              nhev: 15\n",
      "               nit: 22\n",
      "             niter: 22\n",
      "              njev: 15\n",
      "        optimality: 7.675614016643137e-10\n",
      "            status: 1\n",
      "           success: True\n",
      "         tr_radius: 514142.17828960245\n",
      "                 v: [array([-1.34439549,  0.08069263]), array([-6.12001029e-06, -1.38553409e-01, -4.32183906e-06, -2.58760189e-06,\n",
      "       -1.05422968e-01])]\n",
      "                 x: array([2.09713769e-01, 1.40086353e-05, 2.95569338e-01, 4.94696653e-01,\n",
      "       6.23141987e-06]) _end_\n",
      "\n",
      "\n",
      " answer:  [2.09713769e-01 1.40086353e-05 2.95569338e-01 4.94696653e-01\n",
      " 6.23141987e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=0.15000149595255116>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.2459422122855727>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# созданы просто для проверки подсчёта градиентов, что вообще наш метод способен подсчитать тот функционал\n",
    "def rigth_gradient():\n",
    "    global model_\n",
    "    with tf.GradientTape(persistent = True) as g:\n",
    "        g.watch((model_.weights))\n",
    "        functional = model_._Markovitz_functionals_()\n",
    "        grad_ = g.gradient(functional, model_.weights)\n",
    "\n",
    "def grad_numpy(model):\n",
    "    return [2 * np.dot(model.cov_daily_tf.numpy(), model.weights.numpy())[ind_] \\\n",
    "            for ind_ in range(model.weights.shape[0])]\n",
    "\n",
    "### созданеи объектов типа linear_const и bounds ###\n",
    "Q = 0.15 # Риск, допустимый, который нас устраивает \n",
    "linear_const_mark = model_._linear_constraints_(Q) # create a oblect LinearConstraints\n",
    "bounds_mark = model_._bounds_() # create a object of bounds condition\n",
    "W_0 = model_.weights.numpy() # tensor object Variable мы его в numpy переводим(мы его передаём в сам minimize)\n",
    "\n",
    "functionals_Markovitz = _wrapper_marcovitz(model_) # return a functional_obj\n",
    "### Повторяем как было выше для обычной минимизации ###\n",
    "foo_markovitz_obj = function(W_0[:, 0], functionals_Markovitz) # Так как в классе модели марковица вектор представлен в виде (num, 1) -> (num, ) \n",
    "function_mark = foo_markovitz_obj._wrapper()\n",
    "function_mark_grad = foo_markovitz_obj.wrapper_grad()\n",
    "function_mark_hess = foo_markovitz_obj.wrapper_hessian()\n",
    "### Конец создания необходимых функциональных объектов ###\n",
    "\n",
    "### Вызываем решатель ###\n",
    "res_Markovitz = minimize(function_mark, W_0[:, 0], method = 'trust-constr', jac = function_mark_grad,\n",
    "                        hess = function_mark_hess, constraints = [linear_const_mark],\n",
    "                        options = {'verbose': 1}, bounds = bounds_mark)\n",
    "print(\"_output_: \", res_Markovitz, '_end_\\n')\n",
    "print('\\n answer: ', res_Markovitz.x)\n",
    "\n",
    "\n",
    "### change our new parameter ###\n",
    "model_.weights = tf.Variable(res_Markovitz.x[:, np.newaxis])\n",
    "model_.weights\n",
    "\n",
    "model_.__return_total__(), model_._volatility_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.12731777, 0.11479287, 0.13389531, 0.33782186, 0.06027345])>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_(weight_tf):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch((weight_tf))\n",
    "        fun = functionals_Markovitz(weight_tf)\n",
    "        grad_ = g.gradient(fun, weight_tf)\n",
    "        return grad_\n",
    "    \n",
    "    \n",
    "grad_(model_.weights)\n",
    "grad_numpy(model_), grad_(model_.weights)\n",
    "\n",
    "class dop_(object):\n",
    "    \n",
    "    def __init__(self, vec_numpy, _funcion_):\n",
    "        self.tens = tf.Variable(vec_numpy)\n",
    "        self._function_ = _funcion_\n",
    "\n",
    "    def _grad_(self):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch((self.tens))\n",
    "            f = self._function_(self.tens)\n",
    "            grad_ = g.gradient(f, self.tens)\n",
    "        return grad_\n",
    "    \n",
    "cov_matrix_ = model_.cov_daily_tf\n",
    "    \n",
    "def simple_realization(weight):\n",
    "    global cov_matrix_\n",
    "    weight = weight[:, np.newaxis]\n",
    "    return tf.sqrt(tf.matmul(tf.transpose(weight), tf.matmul(cov_matrix_, weight)))[0][0]\n",
    "\n",
    "cls_ = dop_(model_.weights[:,0].numpy(), simple_realization)\n",
    "cls_._grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \n",
      "\n",
      "2.0\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A():\n",
    "    def __init__(self):\n",
    "        self.value = 2.\n",
    "    \n",
    "    def _foo_(self, param):\n",
    "        self.value = param\n",
    "        print(self.value, '\\n')\n",
    "        return self.value ** 2\n",
    "\n",
    "a = A()\n",
    "funct_ = a._foo_\n",
    "funct_(3)\n",
    "a.value\n",
    "def wrap(arg_1):\n",
    "    arg_inner = arg_1 * 2\n",
    "    def _foo_(arg_2):\n",
    "        print( arg_inner )\n",
    "        return arg_1 + arg_2 + arg_inner\n",
    "    return _foo_\n",
    "\n",
    "foo_copy_ = wrap(arg_1 = 1.)\n",
    "\n",
    "\n",
    "def wrapper_new_(funct_obj):\n",
    "    return funct_obj(1)\n",
    "\n",
    "wrapper_new_(foo_copy_)\n",
    "foo_copy_(1.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
